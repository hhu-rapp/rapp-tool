\subsection{Cost Complexity Pruning}

Cost Complexity Pruning ist ein Ansatz zur Vereinfachung von
Entscheidungsbäumen. Nach dem Training des initialen Baumes
werden \(\alpha\)-Werte berechnet, mit denen die Tiefe des Baumes
in einem zweiten Trainingslauf reduziert werden kann. Dies reduziert
das Overfitting (hohe Performance auf dem Trainingset,
niedrige auf dem Testset) und erhöht gleichzeitig die
Generalisierbarkeit und Interpretierbarkeit.
Die Pareto-Front über die Baumtiefe und die Balanced Accuracy
ist über alle berechneten \(\alpha\)-Werte
in \cref{fig:{{label}}-pareto-tree} gegeben.
Im Folgenden sind die Ergebnisse der Pareto-optimalen Bäume im Detail
gelistet.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel=Baumtiefe,
      ylabel=Balanced Accuracy,
      legend style={anchor=south west, at={(0.04,0.04)}}
    ]
      \addplot+[only marks] coordinates {
        {{#nonpareto_coords}}
        ({{depth}}, {{performance}})
        {{/nonpareto_coords}}
      };
      \addplot+[only marks] coordinates {
        {{#pareto_coords}}
        ({{depth}}, {{performance}}) % alpha={{alpha}}
        {{/pareto_coords}}
      };
      \legend{,Paretofront}

    \end{axis}
  \end{tikzpicture}
  \caption{Übersicht der verschiedenen Baumtiefen und Performance-Werte
    im CCP für verschiedene \(\alpha\)-Werte. Die Paretofront ist
    hervorgehoben.}%
  \label{fig:{{label}}-pareto-tree}
\end{figure}

{{#pareto_front}}
\clearpage
\subsubsection{ {{title}} }
{{#figure}}
\Cref{fig:{{label}}-tree} zeigt den trainierten Baum.
{{/figure}}
Die Klassifikations-Performance und die Fairness-Ergebnisse sind in
\cref{tab:{{label}}-performance,tab:{{label}}-fairness}
abgebildet

{{#figure}}
\begin{figure}[ht]
  {{=<< >>=}}
  \includegraphics[width=\linewidth]{<<figure>>}
  <<={{ }}=>>
  \caption{Trainierter Entscheidungsbaum für {{titel}}.}
  \label{fig:{{label}}-tree}
\end{figure}
{{/figure}}


{{{performance_table}}}
{{{fairness_table}}}

{{/pareto_front}}
